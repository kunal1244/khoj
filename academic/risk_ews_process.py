# -*- coding: utf-8 -*-
"""dropout_ews_walkthrough.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_x0RDr7lO8I8mGYY-wTVi5U9OHT3ayqH

# An "Early Warning System" for Student Drop-out Intervention using a Feature-Weighted Nearest Neighbor Model
---------

The following notebook walks you through the development of a supervised learning machine learning tool for the early intervention of potentially failing students.  A sample dataset is included as *student_data.csv* in the same directory.  The accompanying Python module "dropout_ews.py" contains proprietary functions for this notebook.
"""

# LIBRARIES

import numpy as np
import pandas as pd #Successfully installed pandas-0.19.2
import time

from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import make_scorer

import dropout_ews as dews

"""## Data Exploration and Data Preparation
-----------

The sample dataset used in this project is included as student_data.csv. The last column 'passed' is the target/label, all other are feature columns.  The CSV contains a header with the following 30 attributes:

- __school__ : student's school (binary: "GP" or "MS")
- __sex__ : student's sex (binary: "F" - female or "M" - male)
- __age__ : student's age (numeric: from 15 to 22)
- __address__ : student's home address type (binary: "U" - urban or "R" - rural)
- __famsize__ : family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)
- __Pstatus__ : parent's cohabitation status (binary: "T" - living together or "A" - apart)
- __Medu__ : mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to - __9th grade, 3 - secondary education or 4 - higher education)
- __Fedu__ : father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to - __9th grade, 3 - secondary education or 4 - higher education)
- __Mjob__ : mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
- __Fjob__ : father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
- __reason__ : reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")
- __guardian__ : student's guardian (nominal: "mother", "father" or "other")
- __traveltime__ : home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
- __studytime__ : weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
- __failures__ : number of past class failures (numeric: n if 1<=n<3, else 4)
- __schoolsup__ : extra educational support (binary: yes or no)
- __famsup__ : family educational support (binary: yes or no)
- __paid__ : extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
- __activities__ : extra-curricular activities (binary: yes or no)
- __nursery__ : attended nursery school (binary: yes or no)
- __higher__ : wants to take higher education (binary: yes or no)
- __internet__ : Internet access at home (binary: yes or no)
- __romantic__ : with a romantic relationship (binary: yes or no)
- __famrel__ : quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
- __freetime__ : free time after school (numeric: from 1 - very low to 5 - very high)
- __goout__ : going out with friends (numeric: from 1 - very low to 5 - very high)
- __Dalc__ : workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
- __Walc__ : weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
- __health__ : current health status (numeric: from 1 - very bad to 5 - very good)
- __absences__ : number of school absences (numeric: from 0 to 93)

Each student has a target that takes two discrete labels:

- __passed__ : did the student pass the final exam (binary: yes or no)
"""

# READ-IN STUDENT DATA
student_data = pd.read_csv("student_data.csv")
test = pd.read_csv("test.csv")
#print ("Student data read successfully!")


# EXPLORE THE DATA
n_students = student_data.shape[0]
n_features = student_data.shape[1] - 1
n_passed = student_data["risk"].value_counts()[1]
n_failed = student_data["risk"].value_counts()[0]
grad_rate = float(n_passed)/n_students*100

#print "Total number of students: {}".format(n_students)
#print "Number of students who passed: {}".format(n_passed)
#print "Number of students who failed: {}".format(n_failed)
#print "Number of student features: {}".format(n_features)
#print "Graduation rate of the class: {:.2f}%".format(grad_rate)

# EXTRACT FEATURE AND TARGET DATA
feature_cols = list(student_data.columns[:-1])  # all columns but last are features
target_col = student_data.columns[-1]  # last column is the target/label

X_all = student_data[feature_cols]  # feature values for all students
y_all = student_data[target_col]  # corresponding targets/labels

test_example = test[feature_cols]
#print "Feature values:"
#print X_all.head()  # print the first 5 rows
# CREATE DUMMY BINARY VARS FOR ALL CATEGORICAL FEATURES
X_all = dews.preprocess_features(X_all)
test_example = dews.preprocess_features(test_example)
test_example = test_example.iloc[-1:]
#print test_example
#print "Original feature columns: {}".format(n_features)
#print "Final feature columns: {}\n\nList of features: {}".format(len(X_all.columns), list(X_all.columns))

# TEST/TRAIN SPLIT DATA
num_all = student_data.shape[0]  # same as len(student_data)
num_train = 316  # about 80% of the data
num_test = 79

X_train, X_test, y_train, y_test = train_test_split(X_all,y_all,train_size=num_train,test_size=num_test,stratify=y_all)

#print "Data successfully split into Training set and Test set."
#print "Training set: {} samples".format(X_train.shape[0])
#print "Test set: {} samples".format(X_test.shape[0])

f1_scorer = make_scorer(f1_score, pos_label=1)
"""##  Evaluating Other Supervised Learning Models
---------

We choose several supervised learning models that are available in scikit-learn, and evaluate their effectiveness on the sample data set for sake of comparison.  They are:

- Support Vector Machine (SVM) with RBF kernel
- Trimmed Decision Tree
- Bayesian Model

For each model:
- fit the model to the training data
- predict labels (for both training and test sets)
- measure the F<sub>1</sub> score
- repeat this process with different training set sizes (100, 200, 300) while keeping test set constant

We product a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.
"""

# WE VARY THE SIZE OF THE TRAINING SETS FOR THE FOLLOWING MODELS:

# Other Model 1: SUPPORT VECTOR MACHINES

"""## The Feature-Weighted Nearest Neighbor Model
--------

We "feature-weight" a Nearest Neighbors model by paramater-optimizing a decision tree on the full feature set using GridSearch from the SKLearn library.  This allows us to "subset" the full feature set to a set of features that are deemed "important" in the determination of student success.

For academic background on featuring-weighting in nearest neighbor models, see "The Utility of Feature Weighting in Nearest-Neighbor Algorithms", available at http://www.isle.org/~langley/papers/diet.ecml97.pdf.

### Identifying the weighted features with a decision tree
"""

# make scorer

# Subset the Dataset by removing features whose 'importance' is zero, 
# according to a tuned Decision tree in 1.1 
#X_train_subset = X_train[np.nonzero(dt_tuned.feature_importances_)[0].tolist()]
#X_test_subset = X_test[np.nonzero(dt_tuned.feature_importances_)[0].tolist()]

#print "Weighted Features Identified."
#print "Most-important features are {}".format(list(X_train_subset.columns))
parameters = {'max_depth': range(1,15)}
dt = DecisionTreeClassifier()
grid_search = GridSearchCV(dt,parameters,scoring=f1_scorer)
grid_search.fit(X_train,y_train)

# NOTE: with some test/train splits, a DT model will choose max_depth=1 as the best paramter-
# In order retain generality of the subsetting method, we hardcode the max_depth to '3' in dt_tuned classifier.
# dt_tuned = DecisionTreeClassifier(max_depth=grid_search.best_params_['max_depth'])
dt_tuned = DecisionTreeClassifier(max_depth=3)
dt_tuned.fit(X_train,y_train)

# Subset the Dataset by removing features whose 'importance' is zero, 
# according to a tuned Decision tree in 1.1 
sub = np.nonzero(dt_tuned.feature_importances_)[0].tolist()
subset_cols = list(X_train.columns[sub])
X_train_subset = X_train[subset_cols]
X_test_subset = X_test[subset_cols]
test_example_subset = test_example[subset_cols]
#X_train_subset = X_train[sub]
#X_test_subset = X_test[np.nonzero(dt_tuned.feature_importances_)[0].tolist()]

#print "Weighted Features Identified."
#print "Most-important features are {}".format(list(X_train_subset.columns))

"""### Final KNN Model with Feature Weighting, and comparison to KNN Model without Feature Weighting"""

# out-of-the-box KNN
clf_default = KNeighborsClassifier()

# Determine the number of nearest neighbors that optimizes accuracy 
parameters = {'n_neighbors': range(1,30)}
knn = KNeighborsClassifier()
knn_tuned = GridSearchCV(knn,parameters,scoring=f1_scorer)
knn_tuned.fit(X_train_subset,y_train)
clf_tuned = KNeighborsClassifier(n_neighbors=knn_tuned.best_params_['n_neighbors'])

# print performance metrics of default and tuned KNN classifiers on ALL data
#y = dews.train_predict("Full_KNN", X_train, y_train, X_test, y_test, test_example, 100, 200, 300, clf_default, KNeighborsClassifier(n_neighbors=10))
#print ("The predicted result, whether a student will take an extreme step is: "+str(y))
# print performance metrics of default and tuned KNN classifiers on SUBSET of data
y = dews.train_predict("Subset_KNN", X_train_subset, y_train, X_test_subset, y_test, test_example_subset, 100, 200, 300, clf_default, clf_tuned)

print ("The predicted result, whether a student will take an extreme step is: "+str(y))